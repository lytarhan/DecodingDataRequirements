{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulations: How much data do you need for a decoding analysis?\n",
    "##### Leyla Tarhan\n",
    "##### ltarhan@g.harvard.edu\n",
    "##### 4/2020\n",
    "\n",
    "### This is a question that comes up a lot in fMRI world. Say you want to use decoding to detect whether there's a difference between two categories (e.g., big and small objects). How much data (e.g., runs or blocks or subjects) should you collect for each category?\n",
    "\n",
    "### Here, I'm taking a simulation-based approach to ask two questions: \n",
    "\n",
    "### Q1: How much data do you need to detect a real difference between categories?\n",
    "#### To address this, I'm simulate data with the following assumptions:\n",
    "1. There is a real difference between the categories\n",
    "2. The effect size of the difference is XXX\n",
    "3. I'm drawing data from 100 dimensions (in the fMRI case, each dimension is a voxel) and the data for each category is normally distributed (with some offset) in 100-dimensional space\n",
    "\n",
    "Having simulated data that meet these assumptions, I'll iteratively draw a random sample from each category, and do 2-way classification to see if I can separate the categories. The size of the sample I draw will range from 10 to 100. The result will be a plot of the average classification accuracy (on held-out data) over samples of 10-100 data points. This can be interpreted as accuracy at detecting \"true\" positives (a real difference between categories).\n",
    "\n",
    "\n",
    "### Q2. How much data do you need to avoid detecting a difference that isn't real?\n",
    "#### Here, I'll take a similar approach with slightly different assumptions:\n",
    "1. There is NOT a real difference between the categories\n",
    "2. I'll draw data from a single 100-dimensionsal distribution.\n",
    "\n",
    "Again, I'll iteratively draw a random sample from each \"category,\" and do 2-way classification with sample sizes ranging from 10-100 points per category. This time, the result will be a plot of the average classifiction accuracy (which should be at chance, given that there's no real difference between these categories) for each sample size. This can be interpreted as the detection rate for false positives (an apparent difference when there is none)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
